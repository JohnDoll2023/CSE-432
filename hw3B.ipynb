{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3B.ipynb","provenance":[],"authorship_tag":"ABX9TyNJv8h6Cq/qWluCGIHBJwiI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Download MNIST"],"metadata":{"id":"a-dk1YTmy4wC"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"qnbELgnsFxYw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650056826051,"user_tz":240,"elapsed":789,"user":{"displayName":"John Doll","userId":"12045206372755220583"}},"outputId":"457a6736-e46d-47ff-8dbf-275a211135ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading train-images-idx3-ubyte.gz...\n","Downloading t10k-images-idx3-ubyte.gz...\n","Downloading train-labels-idx1-ubyte.gz...\n","Downloading t10k-labels-idx1-ubyte.gz...\n","Download complete.\n","Save complete.\n"]}],"source":["import numpy as np\n","from urllib import request\n","import gzip\n","import pickle\n","\n","filename = [\n","[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n","[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n","[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n","[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n","]\n","\n","def download_mnist():\n","    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n","    for name in filename:\n","        print(\"Downloading \"+name[1]+\"...\")\n","        request.urlretrieve(base_url+name[1], name[1])\n","    print(\"Download complete.\")\n","\n","def save_mnist():\n","    mnist = {}\n","    for name in filename[:2]:\n","        with gzip.open(name[1], 'rb') as f:\n","            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n","    for name in filename[-2:]:\n","        with gzip.open(name[1], 'rb') as f:\n","            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n","    with open(\"mnist.pkl\", 'wb') as f:\n","        pickle.dump(mnist,f)\n","    print(\"Save complete.\")\n","\n","def init():\n","    download_mnist()\n","    save_mnist()\n","#    print ((load()[0]).shape)\n","def load():\n","    with open(\"mnist.pkl\",'rb') as f:\n","        mnist = pickle.load(f)\n","    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n","\n","if __name__ == '__main__':\n","    init()"]},{"cell_type":"markdown","source":["Load function"],"metadata":{"id":"_wk0KB2Fy-Tc"}},{"cell_type":"code","source":["import numpy as np\n","from urllib import request\n","import gzip\n","import math\n","import pickle\n","\n","\n","def grad_softmax_crossentropy(X, y):\n","    m = y.shape[0]\n","    ones_for_answers = np.zeros_like(X)\n","    ones_for_answers[np.arange(len(X)), y] = 1\n","\n","    p = np.exp(X) / np.exp(X).sum(axis=-1, keepdims=True)\n","    return (- ones_for_answers + p) / m\n","\n","def load():\n","    with open(\"mnist.pkl\",'rb') as f:\n","        mnist = pickle.load(f)\n","\n","        training_images, training_labels, testing_images, testing_labels = mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n","        # Normalize the images\n","        training_images.astype('float32')\n","        testing_images.astype('float32')\n","        training_images = training_images / 255\n","        testing_images = testing_images / 255\n","        return training_images, training_labels, testing_images, testing_labels\n","\n","\n","TRimg,TRlab,TSimg,TSlab=load()\n","print(len(TRimg),len(TRlab),len(TSimg),len(TSlab))\n","print(len(TRimg[0]),len(TRlab),len(TSimg[0]),len(TSlab))\n","\n","arr_2d = np. reshape(TRimg[0], (28, 28))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-Mpsry7zCeA","executionInfo":{"status":"ok","timestamp":1650056827917,"user_tz":240,"elapsed":261,"user":{"displayName":"John Doll","userId":"12045206372755220583"}},"outputId":"38836674-4e36-4ffd-e26d-f8573d33b5c7"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["60000 60000 10000 10000\n","784 60000 784 10000\n"]}]},{"cell_type":"markdown","source":["Numpy Implementation"],"metadata":{"id":"BUMgX1cGDCdo"}},{"cell_type":"code","source":["#implement the model\n","#parameters: W: weight matrix\n","# b: bias\n","\"\"\"\n","Assume your input is X\n","Your output is Y\n","\n","For a three layer FC NN, two set of parameters:\n","W: weight matrix for layer 1\n","b: Bias for layer 1\n","W2: weight matrix for layer 2\n","b2: Bias for layer 2\n","W3: weight matrix for layer 3\n","b3: Bias for layer 3\n","\"\"\"\n","\n","D=784 # input size\n","h1 = 200 #first hidden layer\n","h2=50 #second hidden layer\n","K=10  # output size\n","\n","#learning rate\n","step_size=0.1\n","\n","#weight for regularization\n","reg=0.001\n","W=0.01*np.random.randn(D,h1)\n","b=np.zeros((1,h1))\n","W2=0.01*np.random.randn(h1,h2)\n","b2=np.zeros((1,h2))\n","W3=0.01*np.random.randn(h2,K)\n","b3=np.zeros((1,K))\n","\n","#mini batch setting\n","Epoc=10\n","BatchSize=32\n","\n","for i in range(Epoc):\n","  for j in range(0,60000,32):\n","    X=TRimg[j:j+BatchSize]\n","    Y=TRlab[j:j+BatchSize]\n","    num_examples=X.shape[0]\n","\n","    # forward \n","    hidden_layer1=np.maximum(0,np.dot(X,W)+b)\n","    hidden_layer=np.maximum(0,np.dot(hidden_layer1,W2)+b2)\n","    scores=np.dot(hidden_layer,W3)+b3\n","    exp_scores=np.exp(scores)\n","    probs=exp_scores/np.sum(exp_scores,axis=1,keepdims=True)\n","\n","    # Calculate the loss\n","    correct_logprobs=-np.log(probs[range(num_examples),Y])\n","    data_loss=np.sum(correct_logprobs)/num_examples\n","    reg_loss=0.5*reg*np.sum(W*W)+0.5*reg*np.sum(W2*W2)+0.5*reg*np.sum(W3*W3)\n","    loss=data_loss+reg_loss\n","\n","    #backpropagation (softmax+ crossentropy)\n","    dscores=probs\n","    dscores[range(num_examples),Y]-=1\n","    dscores/=num_examples\n","\n","    #Direvative for the third layer (dw3 and db3)\n","    dw3=np.dot(hidden_layer.T,dscores) # direvative for the weight\n","    db3=np.sum(dscores, axis=0,keepdims=True)  # direvative for the bias\n","    dhidden=np.dot(dscores,W3.T)\n","    dhidden[hidden_layer <=0]=0 #Derivitative from Relu\n","\n","    #Direvative for the second layer (dw2 and db2)\n","    dw2=np.dot(hidden_layer1.T,dhidden) # direvative for the weight\n","    db2=np.sum(dhidden, axis=0,keepdims=True)  # direvative for the bias\n","    dhidden1=np.dot(dhidden,W2.T)\n","    dhidden1[hidden_layer1 <=0]=0 #Derivitative from Relu\n","\n","    #Direvative for the first layer (dw and db)\n","    dw= np.dot(X.T,dhidden1)\n","    db=np.sum(dhidden1,axis=0,keepdims=True)\n","\n","    #derivative from regularization\n","    dw3+=reg*W3\n","    dw2+=reg*W2\n","    dw+=reg*W\n","\n","    #update the parameters\n","    W+=-step_size*dw\n","    b+=-step_size*db\n","    W2+=-step_size*dw2\n","    b2+=-step_size*db2  \n","    W3+=-step_size*dw3\n","    b3+=-step_size*db3 \n","\n","\n","#use Test dataset for testing\n","# TSimg and Tslab\n","hidden_layer1=np.maximum(0,np.dot(TSimg,W)+b) #Relu(x*w+b) first layer\n","hidden_layer=np.maximum(0,np.dot(hidden_layer1,W2)+b2) #Relu(x*w+b) second layer\n","scores=np.dot(hidden_layer,W3)+b3 #(hidden*w3+b3)\n","predicted_class=np.argmax(scores,axis=1) #softmax()\n","print(\"training accuracy: %.2f\" % (np.mean(predicted_class==TSlab)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qnq6CqnLRmWB","executionInfo":{"status":"ok","timestamp":1650056869235,"user_tz":240,"elapsed":38970,"user":{"displayName":"John Doll","userId":"12045206372755220583"}},"outputId":"0d1e4093-b121-4bae-d300-a318b401ff7b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["training accuracy: 0.97\n"]}]},{"cell_type":"markdown","source":["Pytorch data"],"metadata":{"id":"rRh0fkYMzQfs"}},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import numpy as np\n","from urllib import request\n","import gzip\n","import math\n","import pickle\n","from time import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.l1 = nn.Linear(784, 200)\n","        self.l2 = nn.Linear(200, 50)\n","        self.l3 = nn.Linear(50,10)\n","\n","\n","    def forward(self, x):\n","        x = F.relu(self.l1(x))\n","        x = F.relu(self.l2(x))\n","        x = F.softmax(self.l3(x))\n","\n","        return x\n","\n","model = Net()\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","epoch=10\n","BatchSize=32\n","for i in range(epoch):\n","  for j in range(0,60000,BatchSize):\n","    X=TRimg[j:j+BatchSize]\n","    X=torch.tensor(X)\n","    Y=TRlab[j:j+BatchSize]\n","    Y=torch.tensor(Y)\n","    num_examples=X.shape[0]\n","    optimizer.zero_grad()\n","    output = model(X.float())\n","    loss=F.cross_entropy(output, Y)\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","#TESTING\n","X=torch.tensor(TSimg)\n","Y=TSlab\n","\n","output = model(X.float())\n","Pred=output.detach().numpy()\n","predicted_class=np.argmax(Pred,axis=1) #softmax()\n","print(predicted_class)\n","print(TSlab)\n","print(\"training accuracy: %.2f\" % (np.mean(predicted_class==TSlab)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0Pa1Qd0zR7d","executionInfo":{"status":"ok","timestamp":1650056909344,"user_tz":240,"elapsed":32002,"user":{"displayName":"John Doll","userId":"12045206372755220583"}},"outputId":"c1a69024-1bdb-4bd6-bcdf-6daace2ae310"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"output_type":"stream","name":"stdout","text":["[7 5 1 ... 4 5 6]\n","[7 2 1 ... 4 5 6]\n","training accuracy: 0.86\n"]}]}]}